{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d824a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSA rows: 30004\n",
      "Label distribution:\n",
      " label\n",
      "1    15002\n",
      "2     7516\n",
      "0     7486\n",
      "Name: count, dtype: int64\n",
      "Train size: 24003\n",
      "Test size: 6001\n",
      "Original rows: 10000\n",
      "Training samples (aspect-level): 24003\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "\n",
    "df = pd.read_json('synthetic_10000.jsonl', lines=True) \n",
    "\n",
    "ASPECTS = [\n",
    "    \"Cost of Living\", \"Healthcare\", \"Education\", \"Personal Security\",\n",
    "    \"Employment\", \"Transportation\", \"Government\", \"Environment\",\n",
    "    \"Social Equality\", \"Taxation\"\n",
    "]\n",
    "\n",
    "labels_expanded = df[\"labels\"].apply(pd.Series)\n",
    "df = pd.concat([df.drop(columns=[\"labels\"]), labels_expanded], axis=1)\n",
    "\n",
    "def transform_to_absa_format(original_df):\n",
    "    transformed_rows = []\n",
    "    \n",
    "    for index, row in original_df.iterrows():\n",
    "        tweet = row['tweet_text']\n",
    "        \n",
    "        active_aspects = []\n",
    "        inactive_aspects = []\n",
    "        \n",
    "        for aspect in ASPECTS:\n",
    "            if row[aspect] != 0:\n",
    "                active_aspects.append(aspect)\n",
    "            else:\n",
    "                inactive_aspects.append(aspect)\n",
    "        \n",
    "        for aspect in active_aspects:\n",
    "            sentiment = row[aspect]\n",
    "            label = 0 if sentiment == -1 else 2\n",
    "            \n",
    "            transformed_rows.append({\n",
    "                'text': tweet, \n",
    "                'aspect': aspect, \n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "        num_negatives_to_sample = len(active_aspects) \n",
    "        if num_negatives_to_sample > 0 and len(inactive_aspects) > 0:\n",
    "            sampled_inactives = random.sample(inactive_aspects, k=min(num_negatives_to_sample, len(inactive_aspects)))\n",
    "            for aspect in sampled_inactives:\n",
    "                transformed_rows.append({\n",
    "                    'text': tweet, \n",
    "                    'aspect': aspect, \n",
    "                    'label': 1 \n",
    "                })\n",
    "            \n",
    "    return pd.DataFrame(transformed_rows)\n",
    "\n",
    "absa_df = transform_to_absa_format(df)\n",
    "print(\"ABSA rows:\", len(absa_df))\n",
    "print(\"Label distribution:\\n\", absa_df[\"label\"].value_counts())\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(absa_df)\n",
    "dataset_dict = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(\"Train size:\", len(dataset_dict[\"train\"]))\n",
    "print(\"Test size:\", len(dataset_dict[\"test\"]))\n",
    "\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "print(f\"Training samples (aspect-level): {len(dataset_dict['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 24003/24003 [00:02<00:00, 8966.93 examples/s]\n",
      "Map: 100%|██████████| 6001/6001 [00:00<00:00, 8949.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3002' max='7505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3002/7505 22:29 < 33:45, 2.22 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.423814</td>\n",
       "      <td>0.830362</td>\n",
       "      <td>0.798556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.459302</td>\n",
       "      <td>0.824529</td>\n",
       "      <td>0.788997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"yangheng/deberta-v3-base-absa-v1.1\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],     \n",
    "        examples['aspect'],   \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=3)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = metric.compute(predictions=pred, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_metric.compute(\n",
    "        predictions=pred,\n",
    "        references=labels,\n",
    "        average=\"macro\"\n",
    "    )[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./absa_finetuned_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,      \n",
    "    weight_decay=0.1,       \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,      \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./final_absa_model\")\n",
    "tokenizer.save_pretrained(\"./final_absa_model\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b29339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './final_absa_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: 'The taxes are killing us, but at least the streets are safe at night.'\n",
      "  Aspect: Cost of Living       -> Negative (0.60)\n",
      "  Aspect: Personal Security    -> Positive (0.91)\n",
      "  Aspect: Taxation             -> Negative (0.81)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "absa_pipeline = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"./final_absa_model\", \n",
    "    tokenizer=\"./final_absa_model\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "def analyze_tweet(text: str, aspect_list):\n",
    "    print(f\"\\nTweet: {text!r}\")\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    for asp in aspect_list:\n",
    "        inputs = tokenizer(text, asp, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.softmax(logits, dim=-1)[0]\n",
    "            cls = torch.argmax(probs).item()\n",
    "            max_p = probs[cls].item()\n",
    "\n",
    "        if cls != 1 and max_p > 0.6:\n",
    "            print(f\"  Aspect: {asp:<20} -> {label_map[cls]} ({max_p:.2f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample = \"The taxes are killing us, but at least the streets are safe at night.\"\n",
    "    analyze_tweet(sample, ASPECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12792b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
